---
title: GenoPred Demo
output: 
  html_document:
    theme: paper
    toc: true
    toc_float: true
    css: styles.css
    includes:
      in_header: header.html
      after_body: footer.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
library(knitr)
```

***

# Overview

This document will provide a demonstration of the GenoPred pipeline. I will cover the following topics:

-   [Installation](#installation)
-   [Run using test data](#run-using-test-data)
-   [Using your own data](#using-your-own-data)
-   [A little about Snakemake](#a-little-about-snakemake)
-   [Run on an HPC](#run-on-an-hpc)
-   [Requesting outputs](#requesting-outputs)
-   [Updating the analysis](#updating-the-analysis)
-   [Finding the desired outputs](#finding-the-desired-outputs)

***

# Installation

There are three steps:

1.  Download GenoPred repository
2.  Create 'base' conda environment
3.  Download dependencies

***

## Step 1: Download GenoPred repository

First, you will need to download the GenoPred repository from GitHub. Open your terminal, go to the directory where you would like the repository to be stored, and clone the repository.

Note. if you are using an high performance cluster (HPC), it is best run the setup in an interactive session (see [here](#Dont-run-on-the-login-node)).

```{bash}
git clone https://github.com/opain/GenoPred.git
```

***

## Step 2: Create conda environment for pipeline

Conda is a software environment management system which is great way for easily downloading and storing software. We will use conda to create an environment that the GenoPred pipeline will run in.

If you don't already have conda installed, we will install it using miniconda.

```{bash}
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
sh Miniconda3-latest-Linux-x86_64.sh
```

I would say `yes` to the default options. You may then need to refresh your workspace to initiate conda, by running `source ~/.bashrc`. You should see `(base)` written in the bottom left of the terminal. Once miniconda installation is complete, you can then delete the `Miniconda3-latest-Linux-x86_64.sh1` file.

Now, go into the `pipeline` folder within the GenoPred repo, and create the conda environment based on the `envs/pipeline.yaml` file. This will create an environment called `genopred` with some essential packages installed.

```{bash}
cd GenoPred/pipeline
conda env create -f envs/pipeline.yaml
```

Now activate the new `genopred` environment.

```{bash}
conda activate genopred
```

***

## Step 3: Download dependencies

To download the dependencies of the GenoPred pipeline we will use our first Snakemake command.

```{bash}
snakemake --restart-times 3 -j 1 --use-conda --conda-frontend mamba get_dependencies
```

This will start by building another conda environment, which the pipeline uses to to perform certain analyses. Then it will download various essential dependencies of the pipeline. In total this process may take ~15 minutes.

**Note.** Check out advice [here](#running-jobs-in-parallel) for running in parallel on an HPC. The command might take some time, so you run via a compute node, and if run interactively, I would suggest using a terminal multiplexer (like tmux) to avoid connection issues (see [here](#avoid-connection-issues)).

<details>

<summary>See explanation of Snakemake command</summary>

-   `--restart-times 3` - Tells Snakemake to try again if a job fails. This is particularly useful when jobs include downloading files from the internet where connections may fail.
-   `-j 1`- This parameter tells Snakemake how many jobs can be run in parallel. 
-   `--use-conda` - This tells Snakemake to use conda environments as specified in the pipeline. This parameter should always be included when using GenoPred.
-   `--conda-frontend mamba` - This tells Snakemake to use mamba when creating new conda environments, which is a faster version of conda. This parameter is only required when running the GenoPred Snakemake for the first time, as the environment only needs to be built once.
-   `get_dependencies` - This is the *rule* we want Snakemake to run. Other useful in GenoPred are described [here](#requesting-specific-outputs). rules The rules included in GenoPred will be described

For more information on Snakemake commands see [here](#basic-Snakemake-commands).

</details>

***

# Run using test data

Now you have installed GenoPred and downloaded its dependencies, we can run the pipeline using the test data.

***

## Step 1: Download the test data

First, we need to download and decompress the test data. Do this within the `GenoPred/pipeline` folder.

```{bash}
# Download from Zenodo
wget -O test_data.tar.gz https://zenodo.org/record/6093584/files/test_data.tar.gz?download=1

# Decompress
tar -xf test_data.tar.gz

# Once decompressed, delete compressed version to save space
rm test_data.tar.gz
```

***

## Step 2: Run the pipeline

The repo automatically contains configuration files to run the pipeline using the test data. Therefore, we just need to run the Snakemake command. See [here](#using-your-own-data) for information on Snakemake configuration files.

```{bash}
snakemake -j1 --use-conda output_all
```

This will run many jobs, testing a large proportion of the pipeline. I would suggest running the pipeline using the test data before moving on to using your own data.

***

# Using your own data

To run the pipeline using your own data, you must specify a new configfile, telling the pipeline what you want it to do, and where the input files are located. Once you have created these input files, you specify the new configfile like this: 

```{bash}
snakemake -j1 --use-conda --configfile new_config.yaml output_all
```

***

## `configfile`

By default, when you run snakemake it reads the `config.yaml` file in the snakemake directory, which lists various parameters, telling it what to do. The parameters in the config files are decribed below:

```{r, eval = T, echo = F, results = 'asis'}
config <- list(
  Parameter = list(
    outdir = list(
      description = 'Directory to save pipeline outputs',
      example = '`test_data/output/test1`',
      note = 'Required'
    ),
    config_file = list(
      description = 'Location of the config file itself',
      example = '`config.yaml`',
      note = 'Required'
    ),
    gwas_list = list(
      description = "Path to `gwas_list` file, listing GWAS sumstats",
      example = '`example_input/gwas_list_example.txt`',
      note = "Set to NA if you don't want to include and GWAS sumstats"
    ),
    score_list = list(
      description = "Path to `score_list` file, listing external score files",
      example = '`example_input/score_list_example.txt`',
      note = "Set to NA if you don't want to include any external score files"
    ),
    target_list = list(
      description = "Path to `target_list` file, listing target datasets",
      example = '`example_input/target_list_example.txt`',
      note = "Set to NA if you don't want to include any target datasets"
    ),
    pgs_methods = list(
      description = 'List of polygenic scoring methods to run',
      example = "`['ptclump','dbslmm']`",
      note = "Options are: `ptclump`, `dbslmm`, `prscs`, `sbayesr`, `lassosum`, `ldpred2`, `megaprs`"
    ),
    testing = list(
      description = 'Directory to save pipeline outputs',
      example = '`chr22`',
      note = "Set to NA to turn off test mode. Set to `chr22` if you want to run the pipeline using only chromosome 22."
    )
  )
)

config_df <- NULL
for (param in names(config$Parameter)) {
  description <- config$Parameter[[param]]$description
  example <- config$Parameter[[param]]$example
  note <- config$Parameter[[param]]$note
  
  # Append each parameter's details to the data frame
  config_df <- rbind(
    config_df,
    data.frame(
      Parameter = paste0("`",param,"`"),
      Description = description,
      Example = example,
      Note = note,
      stringsAsFactors = FALSE
    )
  )
}

print(kable(config_df, 'markdown'))

```

**Note.** If you do not provide a target_list, then only rules that do not require a target_list can be performed, such as GWAS sumstat QC. Similarly, if you do not provide a gwas_list or score_list, only rules that do not require these files can be performed, such as target sample ancestry inference.

***

When using your own data, instead of updating the default `config.yaml` file, I would recommend creating a new config file, and then telling snakemake to use that config file, using the `--configfile` parameter. This allows you to use the GenoPred pipeline with multiple configurations. E.g.

```{bash}
 snakemake -j1 --use-conda --configfile=misc/23andMe/config.yaml output_all
```

***

## `gwas_list`

The `gwas_list` is a white-space delimited text file, providing information of the GWAS summary statistics to be used by the pipeline. The file must have the following columns:

```{r, eval = T, echo = F, results = 'asis'}

gwas_list <- list(
  Column = list(
    name = list(
      example = '`COAD01`',
      description = "ID for the GWAS sumstats. Cannot contain spaces (' ') or hyphens ('-')"
    ),
    path = list(
      example = '`gwas_sumstats/COAD01.gz`',
      description = "File path to the GWAS summary statistics (uncompressed or gzipped)."
    ),
    population = list(
      example = '`EUR`',
      description = "Reference population that the GWAS sample matches best. Options are `AFR` - African, `AMR` = Admixed American, `EAS` = East Asian, `EUR` = European, and `SAS` = South Asian. If you are using a mixed ancestry GWAS, though there are limitations, I would suggest specifying the population that matches the majority of the GWAS sample."
    ),
    n = list(
      example = '`10000`',
      description = "The total sample size of the GWAS. This is required if there is no column indicating sample size in the sumstats. Otherwise, it can be set to `NA`"
    ),
    sampling = list(
      example = '`0.5`',
      description = "The proportion of the GWAS sample that were cases (if outcome is binary - otherwise specify `NA`)"
    ),
    prevalence = list(
      example = '`0.1`',
      description = "The prevelance of the phenotype in the general population (if outcome is binary - otherwise specify `NA`)"
    ),
    mean = list(
      example = '`100`',
      description = "The phenotype mean in the general population (if outcome is continuous, otherwise specify `NA`)"
    ),
    sd = list(
      example = '`15`',
      description = "The phenotype sd in the general population (if outcome is continuous, otherwise specify `NA`)"
    ),
    label = list(
      example = '`\"Coronary Artery Disease\"`',
      description = "A human readable name for the GWAS phenotype. Wrap in double quotes if multiple words. For example, `\"Body Mass Index\"`."
    )
  )
)

gwas_list_df <- NULL
for (column in names(gwas_list$Column)) {
  description <- gwas_list$Column[[column]]$description
  example <- gwas_list$Column[[column]]$example

  # Append each parameter's details to the data frame
  gwas_list_df <- rbind(
    gwas_list_df,
    data.frame(
      Column = column,
      Example = example,
      Description = description,
      stringsAsFactors = FALSE
    )
  )
}

kable(gwas_list_df, 'markdown')

```

**Note. ** The `prevalence` and `sampling` values are used to estimate the SNP-based heritability on the liability scale, as requested by some PGS methods. Furthermore, `prevalence` and `sampling`, or `mean` and `sd` values are used to interpret the polygenic scores on the absolute scale.

***

### GWAS sumstat format

The pipeline can accept GWAS sumstats with a range of header formats. It uses a dictionary to interpret the meaning of certain column names. This is useful but potentially risky. You have two options to ensure the columns are being interpreted correctly:

1. Hope for the best and check the sumstat QC log file to see whether header we correctly interpreted (lazy option but fine in most cases).
2. Check whether the headers in your sumstats correspond to the correct values in the sumstat header dictionary ([here](https://github.com/opain/GenoUtils/blob/main/R/constants.R#L6)), and update as necessary in advance of running the pipeline.

***

The sumstats must contain either RSIDs or chromosome and basepair position information. The sumstats must also contain an effect size, either BETA, odds ratio, log(OR), or a signed Z-score. Either P-values or standard errors must also be present. It is also best if the following are present: sample size per variant, GWAS sample allele frequencies (for the effect allele), and imputation quality metrics.

I would suggest checking the sumstat QC log files, to make to check the number of SNPs after QC is expected.

***

## `score_list`

The `score_list` is a white-space delimited text file, providing information of externally generated score files for polygenic scoring are to be used by the pipeline. The `score_list` should have `name`, `path` and `label` columns, that the `gwas_list` has, except the `path` column should indicate the location of the score file. 

PGS Catalogue score files can be directly downloaded by GenoPred, by using the PGS ID in the `name` column, and setting the `path` column to `NA`. 

**Note.** Externally derived PGS score files may have a poor variant overlap with the default GenoPred reference data, which is restricted to HapMap3 variants. Score files with <75% of variants present in the reference are excluded from downstream target scoring. Several popular PGS methods restrict to HapMap3 variants, so this is not always an issue.

***

### Score file format

The format of the score files should be consistent with the PGS Catalogue header format (https://www.pgscatalog.org/downloads/#scoring_header). GenoPred can read the harmonised and unharmonised column names from PGS Catalogue. It will preferentially use the harmonised columns if they are present. The PGS Catalogue format comments are not required by GenoPred, though they are useful so don't actively remove them. GenoPred allows only one column of effect sizes per score file. GenoPred is lenient, and only requires either the RSIDs, or chromosome  and basepair position columns to be present.

- `rsID` or `hm_rsID` - RSID
- `chr_name` or `hm_chr` - Chromosome number
- `chr_position` or `hm_pos` - Basepair position
- `effect_allele` - Allele corresponding to `effect_weight` column
- `other_allele` - The other allele
- `effect_weight` - The effect size of `effect_allele`

***

## `target_list`

The `target_list` is a white-space delimited text file, providing information of the target datasets to be used by the pipeline. The file must have the following columns:

```{r, eval = T, echo = F, results = 'asis'}

target_list <- list(
  Column = list(
    name = list(
      example = '`test_data/output/test1`',
      description = "ID for the target dataset. Cannot contain spaces (' ') or hyphens ('-')"
    ),
    path = list(
      example = '`imputed_sample_plink1/example`',
      description = "Path to the target genotype data. For type `23andMe`, provide full file path either zipped (`.zip`) or uncompressed (`.txt`). For `type` `plink1`, `bgen`, and `vcf`, per-chromosome genotype data should be provided with the following filename format: `<prefix>.chr<1-22>.<.bed/.bim/.fam/.bgen/.vcf.gz>`. If `type` is `samp_imp_bgen`, the sample file should be called `<prefix>.sample`, and each `.bgen` file should have a corresponding `.bgi` file"
    ),
    type = list(
      example = '`plink1`',
      description = "Format of the target genotype dataset. Either `23andMe`, `plink1`, `bgen`, or `vcf`. `23andMe` = 23andMe formatted data for an individual. `plink1` = Preimputed PLINK1 binary format data (.bed/.bim/.fam). `bgen` = Preimputed Oxford format data (.bgen/.sample). `vcf` = Preimputed gzipped VCF format data (`.vcf.gz`) for a group of individuals."
    ),
    indiv_report = list(
      example = '`T`',
      description = "Logical indicating whether reports for each individual should be generated. Either `T` or `F`. Use with caution if target data contains many individuals, as it will create an .html report for each individual."
    )
  )
)

target_list_df <- NULL
for (column in names(target_list$Column)) {
  description <- target_list$Column[[column]]$description
  example <- target_list$Column[[column]]$example

  # Append each parameter's details to the data frame
  target_list_df <- rbind(
    target_list_df,
    data.frame(
      Column = paste0("`",column,"`"),
      Example = example,
      Description = description,
      stringsAsFactors = FALSE
    )
  )
}

kable(target_list_df, 'markdown')

```

***


# A little about Snakemake

There is full documentation of snakemake [here](https://snakemake.readthedocs.io/en/v7.32.3/), but in this section I will give a brief overview and outline a few commands that are particularly useful.

Snakemake is a python based pipeline tool. It contains lists of rules - Each rule has inputs and outputs. If the user requests a certain output, snakemake will run all the rules that are needed to create that output. 

Importanly, snakemake check the timestamps of input and output files, ensuring the output file is created after the input file was. This helps if you need to rerun your analysis after some changes, and want to make sure the output has been correctly updated.

***

## Updating your analyses

Because snakemake checks timestamps in this way, if you run the GenoPred pipeline, and then change the `configfile`, `gwas_list`, or `target_list`, `score_list`, Snakemake will rerun jobs that might have been affected by the changes. It is quite strict, so you can save time by avoiding reruns of jobs that you know won't be affected. Here are two common examples:

### 1. Adding another PGS method to the configfile

If you modify the configfile, by changing the list of `pgs_methods`, it will trigger a complete rerun of the pipeline by default. However, this might be unnecessarily strict if you just added a PGS method, and the pipeline has already completed runs for some PGS methods that you still want to use. In this scenario, I would use the `-t` parameter to tell snakemake that these jobs have already been completed.

For example, if I had already calculated polygenic scores from the method `ldpred2` in my target datasets, but wanted to include `dbslmm` as well, I would run the following command to avoid Snakemake rerunning the rules required for `ldpred2`:

```{bash}
outdir = test_data/output/test1
name = example_plink1
method = ldpred2

snakemake -j1 -t --use-conda ${outdir}/resources/data/target_checks/${name}/target_pgs-${method}-all_pop.done
```

This will 'touch' the outputs required to generate the requested out, telling snakemake that it doesn't need to do these steps again.

I provide more information on requesting specific outputs [here](#requesting-specific-outputs). I also recommend using the `-p` option when running snakemake, to see what snakemake will do before it runs (see [here](#n) for more information on dry runs).

### 2. Adding another GWAS to the gwas_list

Another common scenario where you might want to update your analysis, is if you have run the pipeline, but then want to include another GWAS in your `gwas_list`. Since the `gwas_list` file will have been updated, snakemake will try and rerun analyses for GWAS that may be unaffected. You might be able to save some time by again using the `-t` option in snakemake to touch certain outputs that do not need to be recreated. For example, say you already ran the pipeline using the gwas called `COAD01`, and the you include another GWAS. You can recreate the unaffected `COAD01` outputs using the following commands:

```{bash}
outdir = test_data/output/test1
name = example_plink1
method = ldpred2
population = EUR
gwas = COAD01

snakemake -j1 -t --use-conda ${outdir}/resources/data/target_checks/${name}/target_pgs-${method}-${population}-${gwas}.done
```

This command will touch the outputs required for the `ldpred2` PGS for the `COAD01` GWAS in the `EUR` population of the target sample `example_plink1`. If there are multiple GWAS, PGS methods and populations then you will need to use a for loop to touch each of the their required outputs.

***

## Useful snakemake parameters

Here are some useful snakemake parameters to run the pipeline.

***

### `--use-conda`

This command tells snakemake to create and use conda environment specified for each rule. This is a handy and reprodible way of installing and running code in a tightly controlled software environment.

This command should always be used when running GenoPred. All rules in GenoPred use the same conda environment, so it only has to be build once.

### `-j`

This command allows to set the number of jobs that can run simultaneously. E.g. `-j 1` will run one job at a time. This is most often what you want if you are running interactively.

### `-n`

This command performs a dry run, where snakemake prints out all the jobs it would run, without actually running them. This is particularly useful if you want to see what would happen if you were to specify a certain output or rule. This helps avoid accidentally triggering 100s of unwanted jobs.

### `-t`

This will 'touch' the output of the jobs requested, meaning their timestamp will be updated, and snakemake will then no longer try and recreate those outputs. This useful if you run the pipeline, change some of the inputs, but want to save time by avoiding rerunning things that you know haven't changed. For example, if you run the pipeline, and want to include an extra GWAS, but you don't want to rerun analyses for the GWAS that have already been run.

### --configfile

This parameter can be used to specify the .yaml file you want snakemake to use as the configuration file. This file is described above (see [here](#configfile)).

```{bash}
snakemake -j1 --use-conda --configfile new_config.yaml output_all
```

### `-p`

This will print the command snakemake will run beneath of the jobs. This is handy if you want to see what the jobs are doing. This is mainly useful when debugging.

***

# Run on an HPC

The GenoPred pipeline can be easily run in parallel using an HPC. Here I outline a few suggestions on how to do this.

***

## Don't run on the login node

HPC's are a shared resource, and the login node is for logging in, not for running analyses. Instead, connect interactively to a compute node before setting up or using the GenoPred pipeline, or submit your snakemake command as a batch job. There are likely time and memory restrictions on the login node, leading to errors, or unhappy colleagues :). Read the documentation for your HPC for more information.

***

## Running jobs in parallel

Snakemake pipelines (such as GenoPred) can be easily parallelised using the `--profile` flag. For this to work, you must first create a .yaml file specifying how to interact with your HPC's job schedular. I have provided an example for users using a SLURM scheduler (`example_input/slurm.yaml`). SLURM users should create a folder called `slurm` in `$HOME/.config/snakemake`, and then copy in the `slurm.yaml`, renaming it to `config.yaml`. More information about profiles in Snakemake can be found [here](https://snakemake.readthedocs.io/en/stable/executing/cli.html#profiles). 

Once you have set up a .yaml for your scheduler, you can tell Snakemake to submit jobs to the scheduler by using the `--profile slurm` parameter, instead of the `-j1` parameter. E.g.

```{bash}
snakemake --profile slurm --use-conda output_all
```

Although, running the snakemake command with the `--profile` flag uses very little memory, I would still suggest running it on a compute node to avoid clogging up the login node.

***

## Avoid connection issues

The pipeline can take hours for certain tasks, so if you are running the snakemake command using interactive session on your HPC, you will likely run into issues due to your connection dropping out, leading to the snakemake analysis to end.

To avoid this, I use a terminal multiplexer, either `tmux` or `screen`. When you are on the login node, start one of these multiplexers. Once inside the multiplexer, start your interactive session. The main reason for using a multiplexer here is that you can reconnect to the session even if your connection stops. There are several other advantages as well. They are really easy to use and will make your life a lot better.

tmux documentation: https://github.com/tmux/tmux/wiki

***

# Requesting outputs

The GenoPred pipeline has many potential outputs. Here is a detailed schematic diagram illustrating the inputs, outputs and processes of the GenoPred pipeline.

![](Images/demo/pipeline_schematic.png)

***

Each of the key outputs from the pipeline can be requested using the corresponding snakemake rule. For example, if I just wanted to obtain QC'd GWAS summary statistics I could run the `prep_sumstat` rule.

```{bash}
snakemake -j1 --use-conda prep_sumstat
```

<details>

<summary>Show table of rules for key outputs</summary>

```{r, eval = T, echo = F, results = 'asis'}

pipeline_dict <- list(
  Type = list(
    Target = list(
      output_all = 'Generates both sample-level and individual-level .html reports',
      sample_report = "Generates sample-level .html reports for target datasets with `format` = 'plink1','bgen', or 'vcf'.",
      indiv_report = 'Generates individual-level .html reports for all individuals in target datasets with `indiv_report = T`',
      ancestry_inference = 'Perform ancestry inference for all target datasets.',
      target_pgs = 'Calculates polygenic scores in all target datasets using score files for all GWAS and all PGS methods.',
      pc_projection = 'Projects reference genetic PCs into all target datasets',
      format_target = 'Harmonises all target datasets with the reference.',
      outlier_detection = 'Perform QC within the target datasets, seperately for each population with N > 100. Includes relatedness estimation, PCA, and population outlier detection.',
      impute_23andme = "Perform genotype imputation of target datasets with `format` = '23andMed'."
    ),
    Reference = list(
      prep_sumstat = 'Performs quality control of all GWAS summary statistics.',
      pgs_prep = 'Prepares scoring files for all GWAS using all PGS methods.',
      ref_pca = 'Performs PCA using reference genotype data.'
    )
  )
)

pipeline_df <- NULL
for (type in names(pipeline_dict$Type)) {
  for (step in names(pipeline_dict$Type[[type]])) {
    description <- pipeline_dict$Type[[type]][[step]]
    pipeline_df <- rbind(pipeline_df, data.frame(Type = type, Rule = step, Description = description, stringsAsFactors = FALSE))
  }
}

print(kable(pipeline_df, 'markdown'))

```

</details>

</br>

***

## Requesting specific outputs

The rules above trigger sets of outputs to be created. For example the `prep_sumstat` rule performs QC of all GWAS in the `gwas_list`. However, it is also possible to request more specific outputs, such as QC'd sumstats for just one of the GWAS in the `gwas_list`. If we had a GWAS with the name `COAD01`, we could request QC'd sumstats for just that GWAS like this:

```{bash}
# Create variables indicating the desired GWAS and the outdir parameter in the config file (by default snakemake reads uses config.yaml)
gwas = COAD01
outdir = test_data/output/test1

# Run snakemake command
snakemake -j1 --use-conda ${outdir}/resources/data/gwas_sumstat/${gwas}/${gwas}-cleaned.gz
```

<details>

<summary>Show table of all available outputs</summary>

```{r, eval = T, echo = F, results = 'asis'}

output_dict <- list(
  type = list(
    target = list(
      group = list(
        `QC'd genotype` = list(
          output = c(
            "{outdir}/{name}/geno/{name}.ref.chr{chr}.bed",
            "{outdir}/resources/data/target_checks/{name}/format_target_all_chr.done"
          ),
          description = c(
            'Specific chromosome of a specific dataset',
            'Specific target dataset'
          )
        ),
        `Imputed genotype` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/impute_23andme-{chr}.done",
            "{outdir}/resources/data/target_checks/{name}/impute_23andme_all_chr.done"
          ),
          description = c(
            'Specific chromosome of a specific dataset',
            'Specific target dataset'
          )
        ),
        `Ancestry Inference` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/ancestry_inference.done"
          ),
          description = c(
            'Specific target dataset'
          )
        ),
        `Within-target QC` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/outlier_detection.done"
          ),
          description = c(
            'Specific target dataset'
          )
        ),
        `Projected PCs` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/pc_projection-{population}.done",
            "{outdir}/resources/data/target_checks/{name}/pc_projection_all_pop.done"
          ),
          description = c(
            'Specific target dataset and specific population',
            'Specific target dataset'
          )
        ),
        `Target PGS` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/target_pgs-{method}-{population}-{gwas}.done",
            "{outdir}/resources/data/target_checks/{name}/target_pgs-{method}-all_gwas-{population}",
            "{outdir}/resources/data/target_checks/{name}/target_pgs-{method}-all_pop.done",
            "{outdir}/resources/data/target_checks/{name}/target_pgs_all_method.done"
          ),
          description = c(
            'Specific target dataset, specific PGS method, specific population and specific GWAS',
            'Specific target dataset, specific PGS method and specific population',
            'Specific target dataset and specific PGS method',
            'Specific target dataset'
          )
        )
      )
    ),
    reference = list(
      group = list(
        `QC'd sumstats` = list(
          output = c(
            "{outdir}/resources/data/gwas_sumstat/{gwas}/{gwas}-cleaned.gz"
          ),
          description = c(
            "Specific GWAS"
          )
        ),
        `PGS score file` = list(
          output = c(
            "{outdir}/resources/data/ref/pgs_score_files/{method}/{gwas}/ref-{gwas}-EUR.scale"
          ),
          description = c(
            'Specific GWAS using specific PGS method'
          )
        ),
        `Reports` = list(
          output = c(
            "{outdir}/resources/data/target_checks/{name}/sample_report.done",
            "{outdir}/resources/data/target_checks/{name}/indiv_report-{id}-report.done",
            "{outdir}/resources/data/target_checks/{name}/indiv_report_all_id.done"
          ),
          description = c(
            'Sample-level report for specific target dataset',
            'Individual-level report for specific individual in specific target dataset',
            'Individual-level report for all individuals in a specific target dataset'
          )
        )
      )
    )
  )
)

# Loop through each level of the list
output_df <- NULL
for (type_name in names(output_dict$type)) {
  for (group_name in names(output_dict$type[[type_name]]$group)) {
    outputs <- output_dict$type[[type_name]]$group[[group_name]]$output
    descriptions <- output_dict$type[[type_name]]$group[[group_name]]$description
    
    # Assuming each 'output' corresponds to a 'description'
    for (i in seq_along(outputs)) {
      output <- outputs[i]
      description <- descriptions[i]
      output_df <- rbind(output_df, data.frame(type = type_name, group = group_name, output = output, description = description, stringsAsFactors = FALSE))
    }
  }
}

print(kable(output_df, 'markdown'))

```

</details>

</br>

***

# Troubleshooting

Please post questions as an issue on the GenoPred GitHub repo [here](https://github.com/opain/GenoPred/issues). If errors occur while running the pipeline, log files will be saved in the `GenoPred/pipeline/logs` folder. If running interactively (i.e. -j1), the error should be printed on the screen.

If there is an unclear error message, feel free to post an issue. A good approach for understanding the issue, is running the failed job interactively, by using the `-p` parameter to print the failed command, and then running interactively to understand the cause of the error. 

***
